{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnzpkQ8-Turl",
        "colab_type": "text"
      },
      "source": [
        "# CS470 - Introduction to Artificial Intelligence\n",
        "## Project : Colorizing grayscale images\n",
        "\n",
        "Authors: Ayoub Mellah 20196411, Quentin Nieloud 20196414, Malek Neila Rostom 20196507, Pablo Chabance 20196417\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3-TraEMTxB0",
        "colab_type": "text"
      },
      "source": [
        "####Connection to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO6IS6ZbTzLp",
        "colab_type": "code",
        "outputId": "e6643b4f-7ea0-4847-e286-3a8438ca4633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive/Colorize'\n",
        "gdrive_data = '/gdrive/My Drive/Colorize/data'"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2FrO9-RT11i",
        "colab_type": "text"
      },
      "source": [
        "####Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QgRSKrrT5kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from PIL import Image as image_pil\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn as nn \n",
        "from skimage import io, color\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.io import imsave\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "import glob\n",
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ZSefARUxSe",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper-Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N_6nJ2aUyw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_epoch = 200\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "training_process = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4bApodUUA97",
        "colab_type": "text"
      },
      "source": [
        "####Construct Data Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7qOvTnUH4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GrayscaleImageFolder(datasets.ImageFolder): #Data preprocessing\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None: #apply transformations if available\n",
        "      img_l = self.transform(img)\n",
        "      img_embed = img.resize((299, 299)) \n",
        "      img_embed = np.asarray(img_embed) #Convert image to numpy array\n",
        "      img_embed = gray2rgb(rgb2gray(img_embed)) #transform rgb image to gray , then grayscale image to rgb ==> values of gray in 3 dimensions \n",
        "      img_embed = torch.from_numpy(img_embed).unsqueeze(0).float() #adds a dimension at the position 0\n",
        "      img_l = np.asarray(img_l) #Convert image to numpy array\n",
        "      img_lab = rgb2lab(img_l) #convert the rgb image to the lab color space\n",
        "      img_lab = (img_lab + 128) / 255 #clip values in the interval domain [0,1]\n",
        "      img_label = img_lab[:, :, 1:] #create the label composed of the a b dimensions of the initial image\n",
        "      img_label = torch.from_numpy(img_label.transpose((2, 0, 1))).float() # transpose the dimensions of the\n",
        "                                                                           # tensor to the right format\n",
        "      img_l = rgb2gray(img_l) #create the graycscale image\n",
        "      img_l = torch.from_numpy(img_l).unsqueeze(0).float() # adds a dimension at the position 0\n",
        "    if self.target_transform is not None: # apply transformations if available\n",
        "      target = self.target_transform(target) \n",
        "    return img_l, img_embed, img_label, target # return the l dimension image, the embeded image for resnet\n",
        "                                               # the a b image (label) and the original image\n",
        "\n",
        "# Training dataset\n",
        "traindir = os.path.join(gdrive_data, 'train')\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
        "train_imagefolder = GrayscaleImageFolder(traindir, train_transforms)\n",
        "train_loader = DataLoader(train_imagefolder, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Validation dataset\n",
        "testdir = os.path.join(gdrive_data, 'test')\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "test_imagefolder = GrayscaleImageFolder(testdir , test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(test_imagefolder, batch_size=10, shuffle=False)\n",
        "\n",
        "# Create generators for the Color Network\n",
        "resnet = resnet152(pretrained=True, progress=True) #loading the resnet network to create the embeddings\n",
        "\n",
        "#Minibatch creation for the coloring network\n",
        "def training_generator():\n",
        "  for batch_l, batch_emb, labels, _ in train_loader:\n",
        "    batch_emb = batch_emb.permute(0,4,2,3,1).squeeze(4) #rearranging the image for resnet\n",
        "    embed = resnet(batch_emb) #getting the embedding\n",
        "    yield([batch_l, embed], labels) #creating the batches for training the network\n",
        "\n",
        "def testing_generator():\n",
        "  for batch_l, batch_emb, labels, _ in test_loader:\n",
        "    batch_emb = batch_emb.permute(0,4,2,3,1).squeeze(4)\n",
        "    embed = resnet(batch_emb)\n",
        "    yield([batch_l, embed], labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAFo16rZU5le",
        "colab_type": "text"
      },
      "source": [
        "#### ColorNet Model Architecture\n",
        "##### Composed of one Encoder, one Decoder and one Fusion Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea87y5y8VBlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENCODER\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    def block(input_size, output_size, stride=False): #Definition of the convolutional layers\n",
        "      layers = [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, stride=2, padding=1)] if stride \\\n",
        "      else [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, padding=1)]\n",
        "      \n",
        "      layers.append(nn.ReLU())\n",
        "      return layers\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      *block(1, 64, True),\n",
        "      *block(64, 128),\n",
        "      *block(128, 128, True),\n",
        "      *block(128, 256),\n",
        "      *block(256, 256, True),\n",
        "      *block(256, 512),\n",
        "      *block(512, 512),\n",
        "      *block(512, 256)\n",
        "    ) #Architecture of the encoder\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vXrOuADVHaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUSION\n",
        "class Fusion(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Fusion, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Conv2d(1256, 256, 1), \n",
        "      nn.ReLU()\n",
        "    )#Convolutional layer followed by a ReLU activation\n",
        "\n",
        "  def forward(self, encoder_output, embed):\n",
        "    base = torch.zeros(10, 1000, 1, 1) #create a tensor full of zero with the following shape\n",
        "    output = embed.unsqueeze(2).unsqueeze(3) #adds dimensions at the position 2 and 3\n",
        "    output[:, :, 0:, :] = base \n",
        "    output[:, :, :, 0:] = base\n",
        "    #make the last 2 dimensions of the output equal 0\n",
        "    output = output.repeat(1, 1, 28, 28) #repeat the last 2 dimension 28 times\n",
        "    output = torch.cat((encoder_output, output), 1) #concatenates the output with the encoder_output\n",
        "    return self.model(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdhFnMwVJjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DECODER\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    def block(input_size, output_size, tanh=False): #definition of the convolutional layers\n",
        "      layers = [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, padding=1)] \n",
        "      layers.append(nn.Tanh()) if tanh else layers.append(nn.ReLU())\n",
        "      return layers\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      *block(256, 128),\n",
        "      nn.Upsample(scale_factor=(2,2)),\n",
        "      *block(128, 64),\n",
        "      nn.Upsample(scale_factor=(2,2)),\n",
        "      *block(64, 32),\n",
        "      *block(32, 16),\n",
        "      *block(16, 2, tanh=True),\n",
        "      nn.Upsample(scale_factor=(2,2))\n",
        "    )#architecture of the decoder\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co8GegtnVMNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COLORNET\n",
        "class ColorNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorNet, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder()\n",
        "    self.fusion = Fusion()\n",
        "    self.decoder = Decoder()\n",
        "\n",
        "  def forward(self, x, embed):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fusion(x, embed)\n",
        "    x = self.decoder(x)\n",
        "    return x\n",
        "\n",
        "#Complete architecture of the network : ENCODER + FUSION + DECODER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyfOy6WBVT6m",
        "colab_type": "text"
      },
      "source": [
        "#### Training ColorNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OhNbmv5VyRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d6aa443-39f5-45ac-b8ea-e2a4391d9ae5"
      },
      "source": [
        "if training_process:\n",
        "\n",
        "  ckpt_file = os.path.join(gdrive_root, 'checkpoint')\n",
        "\n",
        "  net = ColorNet().to(device)\n",
        "  optim = optim.Adam(net.parameters(), learning_rate, weight_decay=0)\n",
        "\n",
        "  train_losses = []\n",
        "\n",
        "  for epoch in range(max_epoch):\n",
        "    net.train()\n",
        "    for inputs, labels in training_generator():\n",
        "\n",
        "      enc_inputs = inputs[0].to(device)\n",
        "      embed_outputs = inputs[1].to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      prediction = net(enc_inputs, embed_outputs)\n",
        "\n",
        "      loss = F.mse_loss(prediction, labels)\n",
        "\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      print('[Epoch:{}/{}] Train Loss:{:.4f}'.format(epoch, max_epoch, loss.item()))\n",
        "\n",
        "    train_losses.append(loss)\n",
        "    if epoch % 10:\n",
        "      torch.save(net.state_dict(), ckpt_file + '/latest.pt')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch:0/200] Train Loss:0.2906\n",
            "[Epoch:1/200] Train Loss:0.2802\n",
            "[Epoch:2/200] Train Loss:0.2405\n",
            "[Epoch:3/200] Train Loss:0.1717\n",
            "[Epoch:4/200] Train Loss:0.0645\n",
            "[Epoch:5/200] Train Loss:0.0483\n",
            "[Epoch:6/200] Train Loss:0.0384\n",
            "[Epoch:7/200] Train Loss:0.0262\n",
            "[Epoch:8/200] Train Loss:0.0210\n",
            "[Epoch:9/200] Train Loss:0.0196\n",
            "[Epoch:10/200] Train Loss:0.0237\n",
            "[Epoch:11/200] Train Loss:0.0378\n",
            "[Epoch:12/200] Train Loss:0.0197\n",
            "[Epoch:13/200] Train Loss:0.0221\n",
            "[Epoch:14/200] Train Loss:0.0275\n",
            "[Epoch:15/200] Train Loss:0.0159\n",
            "[Epoch:16/200] Train Loss:0.0339\n",
            "[Epoch:17/200] Train Loss:0.0196\n",
            "[Epoch:18/200] Train Loss:0.0214\n",
            "[Epoch:19/200] Train Loss:0.0189\n",
            "[Epoch:20/200] Train Loss:0.0156\n",
            "[Epoch:21/200] Train Loss:0.0165\n",
            "[Epoch:22/200] Train Loss:0.0147\n",
            "[Epoch:23/200] Train Loss:0.0162\n",
            "[Epoch:24/200] Train Loss:0.0139\n",
            "[Epoch:25/200] Train Loss:0.0158\n",
            "[Epoch:26/200] Train Loss:0.0136\n",
            "[Epoch:27/200] Train Loss:0.0142\n",
            "[Epoch:28/200] Train Loss:0.0121\n",
            "[Epoch:29/200] Train Loss:0.0139\n",
            "[Epoch:30/200] Train Loss:0.0120\n",
            "[Epoch:31/200] Train Loss:0.0132\n",
            "[Epoch:32/200] Train Loss:0.0116\n",
            "[Epoch:33/200] Train Loss:0.0119\n",
            "[Epoch:34/200] Train Loss:0.0099\n",
            "[Epoch:35/200] Train Loss:0.0121\n",
            "[Epoch:36/200] Train Loss:0.0093\n",
            "[Epoch:37/200] Train Loss:0.0103\n",
            "[Epoch:38/200] Train Loss:0.0090\n",
            "[Epoch:39/200] Train Loss:0.0086\n",
            "[Epoch:40/200] Train Loss:0.0087\n",
            "[Epoch:41/200] Train Loss:0.0076\n",
            "[Epoch:42/200] Train Loss:0.0080\n",
            "[Epoch:43/200] Train Loss:0.0070\n",
            "[Epoch:44/200] Train Loss:0.0069\n",
            "[Epoch:45/200] Train Loss:0.0061\n",
            "[Epoch:46/200] Train Loss:0.0057\n",
            "[Epoch:47/200] Train Loss:0.0053\n",
            "[Epoch:48/200] Train Loss:0.0051\n",
            "[Epoch:49/200] Train Loss:0.0060\n",
            "[Epoch:50/200] Train Loss:0.0065\n",
            "[Epoch:51/200] Train Loss:0.0043\n",
            "[Epoch:52/200] Train Loss:0.0061\n",
            "[Epoch:53/200] Train Loss:0.0041\n",
            "[Epoch:54/200] Train Loss:0.0045\n",
            "[Epoch:55/200] Train Loss:0.0038\n",
            "[Epoch:56/200] Train Loss:0.0039\n",
            "[Epoch:57/200] Train Loss:0.0038\n",
            "[Epoch:58/200] Train Loss:0.0035\n",
            "[Epoch:59/200] Train Loss:0.0040\n",
            "[Epoch:60/200] Train Loss:0.0031\n",
            "[Epoch:61/200] Train Loss:0.0036\n",
            "[Epoch:62/200] Train Loss:0.0030\n",
            "[Epoch:63/200] Train Loss:0.0033\n",
            "[Epoch:64/200] Train Loss:0.0029\n",
            "[Epoch:65/200] Train Loss:0.0030\n",
            "[Epoch:66/200] Train Loss:0.0032\n",
            "[Epoch:67/200] Train Loss:0.0027\n",
            "[Epoch:68/200] Train Loss:0.0027\n",
            "[Epoch:69/200] Train Loss:0.0022\n",
            "[Epoch:70/200] Train Loss:0.0028\n",
            "[Epoch:71/200] Train Loss:0.0024\n",
            "[Epoch:72/200] Train Loss:0.0028\n",
            "[Epoch:73/200] Train Loss:0.0024\n",
            "[Epoch:74/200] Train Loss:0.0021\n",
            "[Epoch:75/200] Train Loss:0.0022\n",
            "[Epoch:76/200] Train Loss:0.0021\n",
            "[Epoch:77/200] Train Loss:0.0021\n",
            "[Epoch:78/200] Train Loss:0.0021\n",
            "[Epoch:79/200] Train Loss:0.0023\n",
            "[Epoch:80/200] Train Loss:0.0020\n",
            "[Epoch:81/200] Train Loss:0.0020\n",
            "[Epoch:82/200] Train Loss:0.0021\n",
            "[Epoch:83/200] Train Loss:0.0019\n",
            "[Epoch:84/200] Train Loss:0.0022\n",
            "[Epoch:85/200] Train Loss:0.0021\n",
            "[Epoch:86/200] Train Loss:0.0018\n",
            "[Epoch:87/200] Train Loss:0.0016\n",
            "[Epoch:88/200] Train Loss:0.0020\n",
            "[Epoch:89/200] Train Loss:0.0026\n",
            "[Epoch:90/200] Train Loss:0.0018\n",
            "[Epoch:91/200] Train Loss:0.0016\n",
            "[Epoch:92/200] Train Loss:0.0018\n",
            "[Epoch:93/200] Train Loss:0.0018\n",
            "[Epoch:94/200] Train Loss:0.0017\n",
            "[Epoch:95/200] Train Loss:0.0016\n",
            "[Epoch:96/200] Train Loss:0.0017\n",
            "[Epoch:97/200] Train Loss:0.0018\n",
            "[Epoch:98/200] Train Loss:0.0019\n",
            "[Epoch:99/200] Train Loss:0.0015\n",
            "[Epoch:100/200] Train Loss:0.0020\n",
            "[Epoch:101/200] Train Loss:0.0019\n",
            "[Epoch:102/200] Train Loss:0.0016\n",
            "[Epoch:103/200] Train Loss:0.0015\n",
            "[Epoch:104/200] Train Loss:0.0016\n",
            "[Epoch:105/200] Train Loss:0.0016\n",
            "[Epoch:106/200] Train Loss:0.0020\n",
            "[Epoch:107/200] Train Loss:0.0023\n",
            "[Epoch:108/200] Train Loss:0.0018\n",
            "[Epoch:109/200] Train Loss:0.0016\n",
            "[Epoch:110/200] Train Loss:0.0019\n",
            "[Epoch:111/200] Train Loss:0.0015\n",
            "[Epoch:112/200] Train Loss:0.0016\n",
            "[Epoch:113/200] Train Loss:0.0015\n",
            "[Epoch:114/200] Train Loss:0.0013\n",
            "[Epoch:115/200] Train Loss:0.0014\n",
            "[Epoch:116/200] Train Loss:0.0018\n",
            "[Epoch:117/200] Train Loss:0.0023\n",
            "[Epoch:118/200] Train Loss:0.0018\n",
            "[Epoch:119/200] Train Loss:0.0015\n",
            "[Epoch:120/200] Train Loss:0.0016\n",
            "[Epoch:121/200] Train Loss:0.0018\n",
            "[Epoch:122/200] Train Loss:0.0018\n",
            "[Epoch:123/200] Train Loss:0.0015\n",
            "[Epoch:124/200] Train Loss:0.0017\n",
            "[Epoch:125/200] Train Loss:0.0015\n",
            "[Epoch:126/200] Train Loss:0.0016\n",
            "[Epoch:127/200] Train Loss:0.0018\n",
            "[Epoch:128/200] Train Loss:0.0015\n",
            "[Epoch:129/200] Train Loss:0.0014\n",
            "[Epoch:130/200] Train Loss:0.0022\n",
            "[Epoch:131/200] Train Loss:0.0021\n",
            "[Epoch:132/200] Train Loss:0.0015\n",
            "[Epoch:133/200] Train Loss:0.0034\n",
            "[Epoch:134/200] Train Loss:0.0043\n",
            "[Epoch:135/200] Train Loss:0.0016\n",
            "[Epoch:136/200] Train Loss:0.0048\n",
            "[Epoch:137/200] Train Loss:0.0041\n",
            "[Epoch:138/200] Train Loss:0.0022\n",
            "[Epoch:139/200] Train Loss:0.0057\n",
            "[Epoch:140/200] Train Loss:0.0024\n",
            "[Epoch:141/200] Train Loss:0.0033\n",
            "[Epoch:142/200] Train Loss:0.0030\n",
            "[Epoch:143/200] Train Loss:0.0016\n",
            "[Epoch:144/200] Train Loss:0.0019\n",
            "[Epoch:145/200] Train Loss:0.0018\n",
            "[Epoch:146/200] Train Loss:0.0020\n",
            "[Epoch:147/200] Train Loss:0.0017\n",
            "[Epoch:148/200] Train Loss:0.0021\n",
            "[Epoch:149/200] Train Loss:0.0015\n",
            "[Epoch:150/200] Train Loss:0.0022\n",
            "[Epoch:151/200] Train Loss:0.0015\n",
            "[Epoch:152/200] Train Loss:0.0020\n",
            "[Epoch:153/200] Train Loss:0.0014\n",
            "[Epoch:154/200] Train Loss:0.0017\n",
            "[Epoch:155/200] Train Loss:0.0015\n",
            "[Epoch:156/200] Train Loss:0.0021\n",
            "[Epoch:157/200] Train Loss:0.0014\n",
            "[Epoch:158/200] Train Loss:0.0020\n",
            "[Epoch:159/200] Train Loss:0.0013\n",
            "[Epoch:160/200] Train Loss:0.0019\n",
            "[Epoch:161/200] Train Loss:0.0013\n",
            "[Epoch:162/200] Train Loss:0.0019\n",
            "[Epoch:163/200] Train Loss:0.0017\n",
            "[Epoch:164/200] Train Loss:0.0016\n",
            "[Epoch:165/200] Train Loss:0.0014\n",
            "[Epoch:166/200] Train Loss:0.0014\n",
            "[Epoch:167/200] Train Loss:0.0015\n",
            "[Epoch:168/200] Train Loss:0.0016\n",
            "[Epoch:169/200] Train Loss:0.0014\n",
            "[Epoch:170/200] Train Loss:0.0019\n",
            "[Epoch:171/200] Train Loss:0.0015\n",
            "[Epoch:172/200] Train Loss:0.0016\n",
            "[Epoch:173/200] Train Loss:0.0014\n",
            "[Epoch:174/200] Train Loss:0.0018\n",
            "[Epoch:175/200] Train Loss:0.0014\n",
            "[Epoch:176/200] Train Loss:0.0019\n",
            "[Epoch:177/200] Train Loss:0.0014\n",
            "[Epoch:178/200] Train Loss:0.0016\n",
            "[Epoch:179/200] Train Loss:0.0014\n",
            "[Epoch:180/200] Train Loss:0.0016\n",
            "[Epoch:181/200] Train Loss:0.0014\n",
            "[Epoch:182/200] Train Loss:0.0018\n",
            "[Epoch:183/200] Train Loss:0.0014\n",
            "[Epoch:184/200] Train Loss:0.0015\n",
            "[Epoch:185/200] Train Loss:0.0013\n",
            "[Epoch:186/200] Train Loss:0.0013\n",
            "[Epoch:187/200] Train Loss:0.0013\n",
            "[Epoch:188/200] Train Loss:0.0014\n",
            "[Epoch:189/200] Train Loss:0.0012\n",
            "[Epoch:190/200] Train Loss:0.0016\n",
            "[Epoch:191/200] Train Loss:0.0015\n",
            "[Epoch:192/200] Train Loss:0.0018\n",
            "[Epoch:193/200] Train Loss:0.0018\n",
            "[Epoch:194/200] Train Loss:0.0017\n",
            "[Epoch:195/200] Train Loss:0.0015\n",
            "[Epoch:196/200] Train Loss:0.0014\n",
            "[Epoch:197/200] Train Loss:0.0013\n",
            "[Epoch:198/200] Train Loss:0.0017\n",
            "[Epoch:199/200] Train Loss:0.0019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1PWS4RPavfP",
        "colab_type": "text"
      },
      "source": [
        "#### Testing ColorNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOwegLKaxhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "ad6342c2-29f5-436d-f273-c232bf644306"
      },
      "source": [
        "ckpt_file = os.path.join(gdrive_root, 'checkpoint') #path of the trained network\n",
        "\n",
        "net = ColorNet().to(device) #creating the model\n",
        "net.load_state_dict(torch.load(ckpt_file + '/latest.pt')) #loading the weitghs\n",
        "\n",
        "for inputs, labels in testing_generator(): #testing the image colorization\n",
        "  enc_inputs = inputs[0].to(device) #encoder input\n",
        "  embed_outputs = inputs[1].to(device) #embedding output\n",
        "  labels = labels.to(device) #label\n",
        "\n",
        "  prediction = net(enc_inputs, embed_outputs) #predicted output of the network\n",
        "\n",
        "  \n",
        "  for i in range(len(prediction)): #reconstruction of the image\n",
        "    cur = np.zeros((224, 224, 3)) #create of an array\n",
        "    tmp = inputs[0][i].permute(1,2,0) #rearring the output to lab domain\n",
        "    tmp = tmp * 100 #normalization\n",
        "    cur[:,:,0] = tmp[:,:,0] #copy of the L domain\n",
        "    cur[:,:,1:] = prediction[i].cpu().detach().permute(1,2,0) * 255 - 128 #copy of the a b prediction to the list followed by normalization\n",
        "    imsave(gdrive_data + \"/output/img_\"+str(i)+\".png\", lab2rgb(cur)) #creating the rgb image and saving it"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}

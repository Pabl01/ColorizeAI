{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Yo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnzpkQ8-Turl",
        "colab_type": "text"
      },
      "source": [
        "# CS470 - Introduction to Artificial Intelligence\n",
        "## Project : Coloring black & white images and video\n",
        "\n",
        "Authors: Ayoub Mellah 20196411, Quentin Nieloud 20196414, Malek Neila Rostom 20196507, Pablo Chabance 20196417\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3-TraEMTxB0",
        "colab_type": "text"
      },
      "source": [
        "####Connection to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO6IS6ZbTzLp",
        "colab_type": "code",
        "outputId": "8fc218d1-228e-42f2-81ed-ad39b94852df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'\n",
        "gdrive_data = '/gdrive/My Drive/IA - Colorize'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2FrO9-RT11i",
        "colab_type": "text"
      },
      "source": [
        "####Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QgRSKrrT5kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from PIL import Image as image_pil\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn as nn \n",
        "from skimage import io, color\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.io import imsave\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "import glob\n",
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-ZSefARUxSe",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper-Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N_6nJ2aUyw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "max_epoch = 50\n",
        "batch_size = 10\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4bApodUUA97",
        "colab_type": "text"
      },
      "source": [
        "####Construct Data Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi7qOvTnUH4L",
        "colab_type": "code",
        "outputId": "b2207d4c-59e4-4051-edc1-64f4b045bcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "class GrayscaleImageFolder(datasets.ImageFolder):\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img_l = self.transform(img)\n",
        "      img_embed = img.resize((299, 299))\n",
        "      img_embed = np.asarray(img_embed)\n",
        "      img_embed = gray2rgb(rgb2gray(img_embed))\n",
        "      img_embed = torch.from_numpy(img_embed).unsqueeze(0).float()\n",
        "      img_l = np.asarray(img_l)\n",
        "      img_lab = rgb2lab(img_l)\n",
        "      img_lab = (img_lab + 128) / 255 #a voir\n",
        "      img_label = img_lab[:, :, 1:]\n",
        "      img_label = torch.from_numpy(img_label.transpose((2, 0, 1))).float()\n",
        "      img_l = rgb2gray(img_l)\n",
        "      img_l = torch.from_numpy(img_l).unsqueeze(0).float()\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    return img_l, img_embed, img_label, target\n",
        "\n",
        "# Training \n",
        "traindir = os.path.join(gdrive_data, 'train_data')\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
        "train_imagefolder = GrayscaleImageFolder(traindir, train_transforms)\n",
        "train_loader = DataLoader(train_imagefolder, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Validation\n",
        "testdir = os.path.join(gdrive_data, 'test_data')\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "test_imagefolder = GrayscaleImageFolder(testdir , test_transforms)\n",
        "test_loader = torch.utils.data.DataLoader(test_imagefolder, batch_size=10, shuffle=False)\n",
        "\n",
        "# Create generators for the Color Network\n",
        "resnet = resnet152(pretrained=True, progress=True)\n",
        "\n",
        "def training_generator():\n",
        "  for batch_l, batch_emb, labels, _ in train_loader:\n",
        "    batch_emb = batch_emb.permute(0,4,2,3,1).squeeze(4)\n",
        "    embed = resnet(batch_emb)\n",
        "    yield([batch_l, embed], labels)\n",
        "\n",
        "def testing_generator():\n",
        "  for batch_l, batch_emb, labels, _ in test_loader:\n",
        "    batch_emb = batch_emb.permute(0,4,2,3,1).squeeze(4)\n",
        "    embed = resnet(batch_emb)\n",
        "    yield([batch_l, embed], labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|██████████| 230M/230M [00:03<00:00, 64.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAFo16rZU5le",
        "colab_type": "text"
      },
      "source": [
        "#### ColorNet Model Architecture\n",
        "##### Composed of one Encoder, one Decoder and one Fusion Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea87y5y8VBlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENCODER\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    def block(input_size, output_size, stride=False):\n",
        "      layers = [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, stride=2, padding=1)] if stride \\\n",
        "      else [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, padding=1)]\n",
        "      \n",
        "      layers.append(nn.ReLU())\n",
        "      return layers\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      *block(1, 64, True),\n",
        "      *block(64, 128),\n",
        "      *block(128, 128, True),\n",
        "      *block(128, 256),\n",
        "      *block(256, 256, True),\n",
        "      *block(256, 512),\n",
        "      *block(512, 512),\n",
        "      *block(512, 256)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vXrOuADVHaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUSION\n",
        "class Fusion(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Fusion, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Conv2d(1256, 256, 1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, encoder_output, embed):\n",
        "    base = torch.zeros(10, 1000, 1, 1)\n",
        "    output = embed.unsqueeze(2).unsqueeze(3)\n",
        "    output[:, :, 0:, :] = base\n",
        "    output[:, :, :, 0:] = base\n",
        "    \n",
        "    #output = embed.unsqueeze(2).unsqueeze(3)\n",
        "    output = output.repeat(1, 1, 28, 28) \n",
        "    output = torch.cat((encoder_output, output), 1)\n",
        "    return self.model(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdhFnMwVJjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DECODER\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    def block(input_size, output_size, tanh=False):\n",
        "      layers = [nn.Conv2d(in_channels=input_size, out_channels=output_size, kernel_size=3, padding=1)]\n",
        "      layers.append(nn.Tanh()) if tanh else layers.append(nn.ReLU())\n",
        "      return layers\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "      *block(256, 128),\n",
        "      nn.Upsample(scale_factor=(2,2)),\n",
        "      *block(128, 64),\n",
        "      nn.Upsample(scale_factor=(2,2)),\n",
        "      *block(64, 32),\n",
        "      *block(32, 16),\n",
        "      *block(16, 2, tanh=True),\n",
        "      nn.Upsample(scale_factor=(2,2))\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co8GegtnVMNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COLORNET\n",
        "class ColorNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ColorNet, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder()\n",
        "    self.fusion = Fusion()\n",
        "    self.decoder = Decoder()\n",
        "\n",
        "  def forward(self, x, embed):\n",
        "    x = self.encoder(x)\n",
        "    x = self.fusion(x, embed)\n",
        "    x = self.decoder(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyfOy6WBVT6m",
        "colab_type": "text"
      },
      "source": [
        "#### Training ColorNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OhNbmv5VyRk",
        "colab_type": "code",
        "outputId": "734d1caa-a6d2-4200-e8ca-3c74c7416f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "ckpt_file = os.path.join(gdrive_data, 'ckpt')\n",
        "\n",
        "net = ColorNet().to(device)\n",
        "optim = optim.Adam(net.parameters(), learning_rate, weight_decay=0)\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  net.train()\n",
        "  for inputs, labels in training_generator():\n",
        "\n",
        "    enc_inputs = inputs[0].to(device)\n",
        "    embed_outputs = inputs[1].to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    prediction = net(enc_inputs, embed_outputs)\n",
        "\n",
        "    loss = F.mse_loss(prediction, labels)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print('[Epoch:{}/{}] Train Loss:{:.4f}'.format(epoch, max_epoch, loss.item()))\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  torch.save(net.state_dict(), ckpt_file + '/latest.pt')\n",
        "  if epoch % 10:\n",
        "    torch.save(net.state_dict(), ckpt_file + '/latest.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch:0/50] Train Loss:0.2554\n",
            "[Epoch:1/50] Train Loss:0.2386\n",
            "[Epoch:2/50] Train Loss:0.1816\n",
            "[Epoch:3/50] Train Loss:0.1730\n",
            "[Epoch:4/50] Train Loss:0.0303\n",
            "[Epoch:5/50] Train Loss:0.0766\n",
            "[Epoch:6/50] Train Loss:0.0274\n",
            "[Epoch:7/50] Train Loss:0.0243\n",
            "[Epoch:8/50] Train Loss:0.0444\n",
            "[Epoch:9/50] Train Loss:0.0190\n",
            "[Epoch:10/50] Train Loss:0.0476\n",
            "[Epoch:11/50] Train Loss:0.0230\n",
            "[Epoch:12/50] Train Loss:0.0328\n",
            "[Epoch:13/50] Train Loss:0.0170\n",
            "[Epoch:14/50] Train Loss:0.0230\n",
            "[Epoch:15/50] Train Loss:0.0188\n",
            "[Epoch:16/50] Train Loss:0.0195\n",
            "[Epoch:17/50] Train Loss:0.0176\n",
            "[Epoch:18/50] Train Loss:0.0151\n",
            "[Epoch:19/50] Train Loss:0.0168\n",
            "[Epoch:20/50] Train Loss:0.0140\n",
            "[Epoch:21/50] Train Loss:0.0154\n",
            "[Epoch:22/50] Train Loss:0.0132\n",
            "[Epoch:23/50] Train Loss:0.0137\n",
            "[Epoch:24/50] Train Loss:0.0127\n",
            "[Epoch:25/50] Train Loss:0.0151\n",
            "[Epoch:26/50] Train Loss:0.0123\n",
            "[Epoch:27/50] Train Loss:0.0146\n",
            "[Epoch:28/50] Train Loss:0.0118\n",
            "[Epoch:29/50] Train Loss:0.0136\n",
            "[Epoch:30/50] Train Loss:0.0115\n",
            "[Epoch:31/50] Train Loss:0.0126\n",
            "[Epoch:32/50] Train Loss:0.0121\n",
            "[Epoch:33/50] Train Loss:0.0126\n",
            "[Epoch:34/50] Train Loss:0.0103\n",
            "[Epoch:35/50] Train Loss:0.0118\n",
            "[Epoch:36/50] Train Loss:0.0099\n",
            "[Epoch:37/50] Train Loss:0.0118\n",
            "[Epoch:38/50] Train Loss:0.0099\n",
            "[Epoch:39/50] Train Loss:0.0106\n",
            "[Epoch:40/50] Train Loss:0.0102\n",
            "[Epoch:41/50] Train Loss:0.0102\n",
            "[Epoch:42/50] Train Loss:0.0095\n",
            "[Epoch:43/50] Train Loss:0.0101\n",
            "[Epoch:44/50] Train Loss:0.0094\n",
            "[Epoch:45/50] Train Loss:0.0098\n",
            "[Epoch:46/50] Train Loss:0.0089\n",
            "[Epoch:47/50] Train Loss:0.0090\n",
            "[Epoch:48/50] Train Loss:0.0089\n",
            "[Epoch:49/50] Train Loss:0.0093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1PWS4RPavfP",
        "colab_type": "text"
      },
      "source": [
        "#### Testing ColorNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOwegLKaxhD",
        "colab_type": "code",
        "outputId": "df356778-9582-4e7b-cd7e-ea044f834098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "ckpt_file = os.path.join(gdrive_data, 'ckpt')\n",
        "\n",
        "net = ColorNet().to(device)\n",
        "net.load_state_dict(torch.load(ckpt_file + '/latest.pt'))\n",
        "\n",
        "for inputs, labels in testing_generator():\n",
        "  enc_inputs = inputs[0].to(device)\n",
        "  embed_outputs = inputs[1].to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  prediction = net(enc_inputs, embed_outputs)\n",
        "  \n",
        "  prediction = prediction\n",
        "\n",
        "  \n",
        "  for i in range(len(prediction)):\n",
        "    cur = np.zeros((224, 224, 3))\n",
        "    tmp = inputs[0][i].permute(1,2,0)\n",
        "    tmp = tmp * 100\n",
        "    cur[:,:,0] = tmp[:,:,0]\n",
        "    cur[:,:,1:] = prediction[i].cpu().detach().permute(1,2,0) * 255 - 128\n",
        "    imsave(gdrive_data + \"/result/img_\"+str(i)+\".png\", lab2rgb(cur))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}